{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JasonChuang88/SVR/blob/main/predict_ATA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hyperopt --quiet"
      ],
      "metadata": {
        "id": "69tyhYY4K_H6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# hide code cell output\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hHYT6eqn6F0J"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as py\n",
        "import scipy.stats as stats\n",
        "from sklearn import metrics, model_selection, preprocessing, svm\n",
        "from typing import Union, Optional\n",
        "# diff: weka use NORMALIZE\n",
        "sc = preprocessing.StandardScaler()"
      ],
      "metadata": {
        "id": "abkESiyPrulD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_split(df: Union[pd.DataFrame, pd.Series], split: float = 0.8, features: int = 1, is_dummy: bool = False):\n",
        "    nd = df.to_numpy()\n",
        "    train_set, test_set = np.split(nd, [int(split * len(nd))])\n",
        "    train_set = train_set.reshape(-1, features)\n",
        "    test_set = test_set.reshape(-1, features)\n",
        "    if not is_dummy:\n",
        "        train_set = sc.fit_transform(train_set)\n",
        "        test_set = sc.transform(test_set)\n",
        "    return train_set, test_set"
      ],
      "metadata": {
        "id": "s0QzklXLjNPZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JfzPYjvb4ocM"
      },
      "outputs": [],
      "source": [
        "def handle_datasets(file_name: str, train_split: float = 0.8) -> list[np.ndarray]:\n",
        "    df = pd.read_csv(f'/content/drive/MyDrive/Colab Notebooks/{file_name}.csv')\n",
        "    # drop row if created_at>ATD\n",
        "    df = df.drop(df[(df['created_at'] > df['ATD'])].index)\n",
        "    df = df.dropna(subset=['ETA','ETD','ATA','ATD','is_rail'])\n",
        "    df = df.drop(columns=['scac_code','firms_code'])\n",
        "    # handle nominal\n",
        "    dummy = df.drop(columns=['ATA','ATD','ETA','ETD','created_at'])\n",
        "    dummy = pd.get_dummies(dummy)\n",
        "    dummy_train, dummy_test = data_split(dummy, split = train_split,features = dummy.shape[1], is_dummy=True)\n",
        "    # exclude 3 columns, others = input features\n",
        "    x = df[df.columns.difference(['ATA','is_rail','POD','POL','svc_term_from','svc_term_to','ship_mode','vessel_name'])]\n",
        "    y = df['ATA']\n",
        "    x_train, x_test = data_split(x, split = train_split,features = x.shape[1])\n",
        "    # concatenate dummy variable and numeric variable\n",
        "    x_train = np.concatenate((dummy_train, x_train), axis=1)\n",
        "    x_test = np.concatenate((dummy_test, x_test), axis=1)\n",
        "    y_train, y_test = data_split(y, split = train_split)\n",
        "    return [x_train, y_train, x_test, y_test]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(datasets: list[np.ndarray], kernel: str = 'rbf') -> None:\n",
        "    x_train, y_train, x_test, y_test = (\n",
        "        datasets[0],\n",
        "        datasets[1],\n",
        "        datasets[2],\n",
        "        datasets[3],\n",
        "    )\n",
        "    model=svm.SVR(kernel=kernel)\n",
        "    print(f'SVR kernel: {kernel}')\n",
        "    model.fit(x_train, y_train.ravel())\n",
        "    # training\n",
        "    y_train_pred=model.predict(x_train)\n",
        "    y_train_pred = sc.inverse_transform(y_train_pred.reshape(-1, 1))\n",
        "    y_train = sc.inverse_transform(y_train)\n",
        "    rmse = metrics.mean_squared_error(y_train, y_train_pred, squared=False)\n",
        "    mae = metrics.mean_absolute_error(y_train, y_train_pred)\n",
        "    r2 = metrics.r2_score(y_train, y_train_pred)\n",
        "    print(f'training RMSE: {rmse}, MAE: {mae}, R2 score: {r2}')\n",
        "    # testing\n",
        "    y_test_pred=model.predict(x_test)\n",
        "    y_test_pred = sc.inverse_transform(y_test_pred.reshape(-1, 1))\n",
        "    y_test = sc.inverse_transform(y_test)\n",
        "    rmse = metrics.mean_squared_error(y_test, y_test_pred, squared=False)\n",
        "    mae = metrics.mean_absolute_error(y_test, y_test_pred)\n",
        "    r2 = metrics.r2_score(y_test, y_test_pred)\n",
        "    print(f'testing RMSE: {rmse}, MAE: {mae}, R2 score: {r2}')\n",
        "    residual = y_test - y_test_pred\n",
        "    plt.clf()\n",
        "    plt.hist(residual, bins=40, range=(-50,50), facecolor=\"blue\", edgecolor=\"black\", alpha=0.7, density=False)\n",
        "    plt.xlabel(\"residual\")\n",
        "    plt.ylabel(\"Probability\")\n",
        "    plt.show(block=False)"
      ],
      "metadata": {
        "id": "aZ-Ho7uzDvz0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def algorithm(params) -> dict:\n",
        "    # options: training_1000, training_10000, CMDU_training_2000, EGLV_training_4000, COSU_training_4000\n",
        "    file_name = \"EGLV_training_4000\"\n",
        "    x_train, y_train, x_test, y_test = handle_datasets(file_name,0.8)\n",
        "    model=svm.SVR(**params)\n",
        "    model.fit(x_train, y_train.ravel())\n",
        "    # testing\n",
        "    y_test_pred=model.predict(x_test)\n",
        "    y_test_pred = sc.inverse_transform(y_test_pred.reshape(-1, 1))\n",
        "    y_test = sc.inverse_transform(y_test)\n",
        "    rmse = metrics.mean_squared_error(y_test, y_test_pred, squared=False)\n",
        "    mae = metrics.mean_absolute_error(y_test, y_test_pred)\n",
        "    r2 = metrics.r2_score(y_test, y_test_pred)\n",
        "    return {\"loss\": mae, \"status\": STATUS_OK}"
      ],
      "metadata": {
        "id": "J8LIXdvwY7tz"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "space = {\n",
        "    \"kernel\": hp.choice(\"kernel\", ['linear', 'poly', 'rbf']),\n",
        "    \"gamma\": hp.choice(\"gamma\", ['scale', 'auto']),\n",
        "    \"C\": hp.quniform(\"C\", 1, 100, 1),\n",
        "    \"max_iter\": hp.choice(\"max_iter\", [-1, 1, 10,100,1000]),\n",
        "    \"epsilon\": hp.choice(\"epsilon\", [0.001, 0.01, 0.1, 1]),\n",
        "    \"shrinking\": hp.choice(\"shrinking\", [True, False]),\n",
        "}"
      ],
      "metadata": {
        "id": "qkR8YQ6nXZfD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperopt import tpe, hp, fmin, STATUS_OK,Trials\n",
        "from hyperopt.pyll.base import scope\n",
        "import warnings\n",
        "if __name__ == \"__main__\":\n",
        "    warnings.filterwarnings('ignore')\n",
        "    # options: training_1000, training_10000, CMDU_training_2000, EGLV_training_4000, COSU_training_4000\n",
        "    trials = Trials()\n",
        "    best = fmin(fn=algorithm,\n",
        "            space=space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=100,\n",
        "            trials = trials)\n",
        "    print(\"Best: {}\".format(best))\n",
        "    print(trials.results)\n",
        "    print(trials.losses())\n",
        "    print(trials.statuses())\n",
        "    '''\n",
        "    file_name = \"EGLV_training_4000\"\n",
        "    data = handle_datasets(file_name=file_name,train_split=0.8)\n",
        "    kernel_list = ['linear', 'poly', 'rbf']\n",
        "    for kernel in kernel_list:\n",
        "        train(data, kernel=kernel)\n",
        "    '''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZy7bywzGFkn",
        "outputId": "7bbdad4c-43e7-4fb3-d7b0-b45d2a73c060"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████| 100/100 [00:08<00:00, 11.47trial/s, best loss: 0.5426957361017452]\n",
            "Best: {'C': 2.0, 'epsilon': 1, 'gamma': 0, 'kernel': 0, 'max_iter': 4, 'shrinking': 0}\n",
            "[{'loss': 2.7103509728093176, 'status': 'ok'}, {'loss': 8.422457962828835, 'status': 'ok'}, {'loss': 5.598710220076444, 'status': 'ok'}, {'loss': 14.258548773759292, 'status': 'ok'}, {'loss': 5.5527587291879765, 'status': 'ok'}, {'loss': 2.9897955722254985, 'status': 'ok'}, {'loss': 2.7103509728093176, 'status': 'ok'}, {'loss': 6.516918317850394, 'status': 'ok'}, {'loss': 2.5082061979249106, 'status': 'ok'}, {'loss': 4.419700420026611, 'status': 'ok'}, {'loss': 8.127957558118961, 'status': 'ok'}, {'loss': 11.080831973144676, 'status': 'ok'}, {'loss': 5.797461199975137, 'status': 'ok'}, {'loss': 8.127957558118961, 'status': 'ok'}, {'loss': 3.72347513463791, 'status': 'ok'}, {'loss': 6.3111695744944, 'status': 'ok'}, {'loss': 5.598710220076444, 'status': 'ok'}, {'loss': 3.466659103418548, 'status': 'ok'}, {'loss': 14.258548773759292, 'status': 'ok'}, {'loss': 9.449100833013535, 'status': 'ok'}, {'loss': 1.6561153303319351, 'status': 'ok'}, {'loss': 2.5082061979249106, 'status': 'ok'}, {'loss': 1.3064907429052746, 'status': 'ok'}, {'loss': 0.6246475483492612, 'status': 'ok'}, {'loss': 0.6222046267200337, 'status': 'ok'}, {'loss': 0.6240200215484161, 'status': 'ok'}, {'loss': 0.619384609968079, 'status': 'ok'}, {'loss': 0.619384609968079, 'status': 'ok'}, {'loss': 0.6233079388619297, 'status': 'ok'}, {'loss': 0.641165893175246, 'status': 'ok'}, {'loss': 0.623123584108701, 'status': 'ok'}, {'loss': 0.6234306143217239, 'status': 'ok'}, {'loss': 0.6225155845537744, 'status': 'ok'}, {'loss': 1.792490243439396, 'status': 'ok'}, {'loss': 0.6276945892919635, 'status': 'ok'}, {'loss': 6.365616116157538, 'status': 'ok'}, {'loss': 0.619384609968079, 'status': 'ok'}, {'loss': 0.6256275549200615, 'status': 'ok'}, {'loss': 6.9217797030541, 'status': 'ok'}, {'loss': 1.6143349755751646, 'status': 'ok'}, {'loss': 9.570896857082639, 'status': 'ok'}, {'loss': 2.9897955722254985, 'status': 'ok'}, {'loss': 6.894132090378096, 'status': 'ok'}, {'loss': 2.7103509728093176, 'status': 'ok'}, {'loss': 0.6234702127745746, 'status': 'ok'}, {'loss': 6.3255477682689305, 'status': 'ok'}, {'loss': 9.449100833013535, 'status': 'ok'}, {'loss': 6.3111695744944, 'status': 'ok'}, {'loss': 5.5527587291879765, 'status': 'ok'}, {'loss': 0.623823301455264, 'status': 'ok'}, {'loss': 14.258548773759292, 'status': 'ok'}, {'loss': 1.6561153303319351, 'status': 'ok'}, {'loss': 33.810901886523965, 'status': 'ok'}, {'loss': 1.3064907429052746, 'status': 'ok'}, {'loss': 4.419700420026611, 'status': 'ok'}, {'loss': 1.5372322937294705, 'status': 'ok'}, {'loss': 1.6561153303319351, 'status': 'ok'}, {'loss': 11.63215081775333, 'status': 'ok'}, {'loss': 4.419700420026611, 'status': 'ok'}, {'loss': 1.6326283929716934, 'status': 'ok'}, {'loss': 9.449100833013535, 'status': 'ok'}, {'loss': 1.5271896178976907, 'status': 'ok'}, {'loss': 1.6691537216314385, 'status': 'ok'}, {'loss': 3.525081733000784, 'status': 'ok'}, {'loss': 4.419700420026611, 'status': 'ok'}, {'loss': 0.6233079388619297, 'status': 'ok'}, {'loss': 0.6205034292793858, 'status': 'ok'}, {'loss': 0.6231332940658684, 'status': 'ok'}, {'loss': 0.623123584108701, 'status': 'ok'}, {'loss': 0.6244194560721233, 'status': 'ok'}, {'loss': 0.6235698451507503, 'status': 'ok'}, {'loss': 0.6233655907800877, 'status': 'ok'}, {'loss': 0.641165893175246, 'status': 'ok'}, {'loss': 1.3064907429052746, 'status': 'ok'}, {'loss': 0.8074419904105771, 'status': 'ok'}, {'loss': 9.570896857082639, 'status': 'ok'}, {'loss': 1.4979774302244895, 'status': 'ok'}, {'loss': 1.6691537216314385, 'status': 'ok'}, {'loss': 0.622514755929375, 'status': 'ok'}, {'loss': 6.849051270162572, 'status': 'ok'}, {'loss': 0.5426957361017452, 'status': 'ok'}, {'loss': 0.8291691608891769, 'status': 'ok'}, {'loss': 4.3411162681960205, 'status': 'ok'}, {'loss': 1.7717880673161424, 'status': 'ok'}, {'loss': 3.4666504264894535, 'status': 'ok'}, {'loss': 1.7717880673161424, 'status': 'ok'}, {'loss': 0.9402188903750849, 'status': 'ok'}, {'loss': 4.360370192547388, 'status': 'ok'}, {'loss': 2.9897955722254985, 'status': 'ok'}, {'loss': 5.598710220076444, 'status': 'ok'}, {'loss': 3.4666504264894535, 'status': 'ok'}, {'loss': 1.6691537216314385, 'status': 'ok'}, {'loss': 4.419700420026611, 'status': 'ok'}, {'loss': 6.45002493114176, 'status': 'ok'}, {'loss': 8.231140592315422, 'status': 'ok'}, {'loss': 3.9562749847251766, 'status': 'ok'}, {'loss': 4.419700420026611, 'status': 'ok'}, {'loss': 2.9897955722254985, 'status': 'ok'}, {'loss': 1.6561153303319351, 'status': 'ok'}, {'loss': 6.298005281145035, 'status': 'ok'}]\n",
            "[2.7103509728093176, 8.422457962828835, 5.598710220076444, 14.258548773759292, 5.5527587291879765, 2.9897955722254985, 2.7103509728093176, 6.516918317850394, 2.5082061979249106, 4.419700420026611, 8.127957558118961, 11.080831973144676, 5.797461199975137, 8.127957558118961, 3.72347513463791, 6.3111695744944, 5.598710220076444, 3.466659103418548, 14.258548773759292, 9.449100833013535, 1.6561153303319351, 2.5082061979249106, 1.3064907429052746, 0.6246475483492612, 0.6222046267200337, 0.6240200215484161, 0.619384609968079, 0.619384609968079, 0.6233079388619297, 0.641165893175246, 0.623123584108701, 0.6234306143217239, 0.6225155845537744, 1.792490243439396, 0.6276945892919635, 6.365616116157538, 0.619384609968079, 0.6256275549200615, 6.9217797030541, 1.6143349755751646, 9.570896857082639, 2.9897955722254985, 6.894132090378096, 2.7103509728093176, 0.6234702127745746, 6.3255477682689305, 9.449100833013535, 6.3111695744944, 5.5527587291879765, 0.623823301455264, 14.258548773759292, 1.6561153303319351, 33.810901886523965, 1.3064907429052746, 4.419700420026611, 1.5372322937294705, 1.6561153303319351, 11.63215081775333, 4.419700420026611, 1.6326283929716934, 9.449100833013535, 1.5271896178976907, 1.6691537216314385, 3.525081733000784, 4.419700420026611, 0.6233079388619297, 0.6205034292793858, 0.6231332940658684, 0.623123584108701, 0.6244194560721233, 0.6235698451507503, 0.6233655907800877, 0.641165893175246, 1.3064907429052746, 0.8074419904105771, 9.570896857082639, 1.4979774302244895, 1.6691537216314385, 0.622514755929375, 6.849051270162572, 0.5426957361017452, 0.8291691608891769, 4.3411162681960205, 1.7717880673161424, 3.4666504264894535, 1.7717880673161424, 0.9402188903750849, 4.360370192547388, 2.9897955722254985, 5.598710220076444, 3.4666504264894535, 1.6691537216314385, 4.419700420026611, 6.45002493114176, 8.231140592315422, 3.9562749847251766, 4.419700420026611, 2.9897955722254985, 1.6561153303319351, 6.298005281145035]\n",
            "['ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok', 'ok']\n"
          ]
        }
      ]
    }
  ]
}